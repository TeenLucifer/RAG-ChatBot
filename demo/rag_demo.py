# -*- coding: utf-8 -*-
import json
import time
from pathlib import Path
from typing import List, Dict
import re
import streamlit as st
import chromadb
from llama_index.core import VectorStoreIndex, StorageContext, Settings, get_response_synthesizer
from llama_index.core.schema import TextNode
from llama_index.llms.huggingface import HuggingFaceLLM
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.core import PromptTemplate
from llama_index.core.postprocessor import SentenceTransformerRerank  # æ–°å¢é‡æ’åºç»„ä»¶

QA_TEMPLATE = (
    "<|im_start|>system\n"
    "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ³•å¾‹åŠ©æ‰‹ï¼Œè¯·ä¸¥æ ¼æ ¹æ®ä»¥ä¸‹æ³•å¾‹æ¡æ–‡å›ç­”é—®é¢˜ï¼š\n"
    "ç›¸å…³æ³•å¾‹æ¡æ–‡ï¼š\n{context_str}\n<|im_end|>\n"
    "<|im_start|>user\n{query_str}<|im_end|>\n"
    "<|im_start|>assistant\n"
)

response_template = PromptTemplate(QA_TEMPLATE)

# ================== Streamlité¡µé¢é…ç½® ==================
st.set_page_config(
    page_title="æ™ºèƒ½åŠ³åŠ¨æ³•å’¨è¯¢åŠ©æ‰‹",
    page_icon="âš–ï¸",
    layout="centered",
    initial_sidebar_state="auto"
)

def disable_streamlit_watcher():
    """Patch Streamlit to disable file watcher"""
    def _on_script_changed(_):
        return
        
    from streamlit import runtime
    runtime.get_instance()._on_script_changed = _on_script_changed

# ================== é…ç½®åŒº ==================
class Config:
    EMBED_MODEL_PATH = "/root/Documents/model_modelscope/thomas/text2vec-base-chinese"
    LLM_MODEL_PATH = "/root/Documents/model_modelscope/Qwen/Qwen1.5-1.8B-Chat"
    RERANK_MODEL_PATH = "/root/Documents/model_modelscope/BAAI/bge-reranker-large"  # æ–°å¢é‡æ’åºæ¨¡å‹è·¯å¾„

    DATA_DIR = "/root/Documents/demo20-24/rag_docs"
    VECTOR_DB_DIR = "/root/Documents/demo20-24/chroma_db"
    PERSIST_DIR = "/root/Documents/demo20-24/storage"

    COLLECTION_NAME = "chinese_labor_laws"
    TOP_K = 10
    RERANK_TOP_K = 3  # æ–°å¢é‡æ’åºçš„top_kå‚æ•°

# ================== ç¼“å­˜èµ„æºåˆå§‹åŒ– ==================
@st.cache_resource(show_spinner="åˆå§‹åŒ–æ¨¡å‹ä¸­...")
# ================== åˆå§‹åŒ–æ¨¡å‹ ==================
def init_models():
    """åˆå§‹åŒ–æ¨¡å‹å¹¶éªŒè¯"""
    # Embeddingæ¨¡å‹
    embed_model = HuggingFaceEmbedding(
        model_name=Config.EMBED_MODEL_PATH
    )
    
    # LLM
    # TODO: è¿™é‡Œå¯ä»¥æ¢æˆåœ¨çº¿æ¨¡å‹, ä¸ç”¨æœ¬åœ°çš„
    llm = HuggingFaceLLM(
        model_name=Config.LLM_MODEL_PATH,
        tokenizer_name=Config.LLM_MODEL_PATH,
        model_kwargs={
            "trust_remote_code": True
        },
        tokenizer_kwargs={"trust_remote_code": True},
        generate_kwargs={"temperature": 0.3}
    )
    #llm = OpenAILike(
    #    model="/home/cw/llms/Qwen/Qwen1.5-1.8B-Chat",
    #    api_base="http://localhost:8000/v1",
    #    api_key="fake",
    #    context_window=4096,
    #    is_chat_model=True,
    #    is_function_calling_model=False,
    #)

    # åˆå§‹åŒ–é‡æ’åºå™¨ï¼ˆæ–°å¢ï¼‰
    reranker = SentenceTransformerRerank(
        model=Config.RERANK_MODEL_PATH,
        top_n=Config.RERANK_TOP_K
    )

    Settings.embed_model = embed_model
    Settings.llm = llm

    # éªŒè¯æ¨¡å‹
    test_embedding = embed_model.get_text_embedding("æµ‹è¯•æ–‡æœ¬")
    print(f"Embeddingç»´åº¦éªŒè¯ï¼š{len(test_embedding)}")
    
    return embed_model, llm, reranker

# ================== æ•°æ®å¤„ç† ==================
def load_and_validate_json_files(data_dir: str) -> List[Dict]:
    """åŠ è½½å¹¶éªŒè¯JSONæ³•å¾‹æ–‡ä»¶"""
    json_files = list(Path(data_dir).glob("*.json"))
    assert json_files, f"æœªæ‰¾åˆ°JSONæ–‡ä»¶äº {data_dir}"
    
    all_data = []
    for json_file in json_files:
        with open(json_file, 'r', encoding='utf-8') as f:
            try:
                data = json.load(f)
                # éªŒè¯æ•°æ®ç»“æ„
                if not isinstance(data, list):
                    raise ValueError(f"æ–‡ä»¶ {json_file.name} æ ¹å…ƒç´ åº”ä¸ºåˆ—è¡¨")
                for item in data:
                    if not isinstance(item, dict):
                        raise ValueError(f"æ–‡ä»¶ {json_file.name} åŒ…å«éå­—å…¸å…ƒç´ ")
                    for k, v in item.items():
                        if not isinstance(v, str):
                            raise ValueError(f"æ–‡ä»¶ {json_file.name} ä¸­é”® '{k}' çš„å€¼ä¸æ˜¯å­—ç¬¦ä¸²")
                all_data.extend({
                    "content": item,
                    "metadata": {"source": json_file.name}
                } for item in data)
            except Exception as e:
                raise RuntimeError(f"åŠ è½½æ–‡ä»¶ {json_file} å¤±è´¥: {str(e)}")
    
    print(f"æˆåŠŸåŠ è½½ {len(all_data)} ä¸ªæ³•å¾‹æ–‡ä»¶æ¡ç›®")
    return all_data

def create_nodes(raw_data: List[Dict]) -> List[TextNode]:
    """æ·»åŠ IDç¨³å®šæ€§ä¿éšœ"""
    nodes = []
    for entry in raw_data:
        law_dict = entry["content"]
        source_file = entry["metadata"]["source"]
        
        for full_title, content in law_dict.items():
            # ç”Ÿæˆç¨³å®šIDï¼ˆé¿å…é‡å¤ï¼‰
            node_id = f"{source_file}::{full_title}"
            
            parts = full_title.split(" ", 1)
            law_name = parts[0] if len(parts) > 0 else "æœªçŸ¥æ³•å¾‹"
            article = parts[1] if len(parts) > 1 else "æœªçŸ¥æ¡æ¬¾"
            
            node = TextNode(
                text=content,
                id_=node_id,  # æ˜¾å¼è®¾ç½®ç¨³å®šID
                metadata={
                    "law_name": law_name,
                    "article": article,
                    "full_title": full_title,
                    "source_file": source_file,
                    "content_type": "legal_article"
                }
            )
            nodes.append(node)
    
    print(f"ç”Ÿæˆ {len(nodes)} ä¸ªæ–‡æœ¬èŠ‚ç‚¹ï¼ˆIDç¤ºä¾‹ï¼š{nodes[0].id_}ï¼‰")
    return nodes

# ================== å‘é‡å­˜å‚¨ ==================
@st.cache_resource(show_spinner="åŠ è½½çŸ¥è¯†åº“ä¸­...")
def init_vector_store(nodes: List[TextNode]) -> VectorStoreIndex:
    chroma_client = chromadb.PersistentClient(path=Config.VECTOR_DB_DIR)
    chroma_collection = chroma_client.get_or_create_collection(
        name=Config.COLLECTION_NAME,
        metadata={"hnsw:space": "cosine"}
    )

    # ç¡®ä¿å­˜å‚¨ä¸Šä¸‹æ–‡æ­£ç¡®åˆå§‹åŒ–
    storage_context = StorageContext.from_defaults(
        vector_store=ChromaVectorStore(chroma_collection=chroma_collection)
    )

    # åˆ¤æ–­æ˜¯å¦éœ€è¦æ–°å»ºç´¢å¼•
    if chroma_collection.count() == 0 and nodes is not None:
        print(f"åˆ›å»ºæ–°ç´¢å¼•ï¼ˆ{len(nodes)}ä¸ªèŠ‚ç‚¹ï¼‰...")
        
        # æ˜¾å¼å°†èŠ‚ç‚¹æ·»åŠ åˆ°å­˜å‚¨ä¸Šä¸‹æ–‡
        storage_context.docstore.add_documents(nodes)  
        
        index = VectorStoreIndex(
            nodes,
            storage_context=storage_context,
            show_progress=True
        )
        # åŒé‡æŒä¹…åŒ–ä¿éšœ
        storage_context.persist(persist_dir=Config.PERSIST_DIR)
        index.storage_context.persist(persist_dir=Config.PERSIST_DIR)  # <-- æ–°å¢
    else:
        print("åŠ è½½å·²æœ‰ç´¢å¼•...")
        storage_context = StorageContext.from_defaults(
            persist_dir=Config.PERSIST_DIR,
            vector_store=ChromaVectorStore(chroma_collection=chroma_collection)
        )
        index = VectorStoreIndex.from_vector_store(
            storage_context.vector_store,
            storage_context=storage_context,
            embed_model=Settings.embed_model
        )

    # å®‰å…¨éªŒè¯
    print("\nå­˜å‚¨éªŒè¯ç»“æœï¼š")
    doc_count = len(storage_context.docstore.docs)
    print(f"DocStoreè®°å½•æ•°ï¼š{doc_count}")
    
    if doc_count > 0:
        sample_key = next(iter(storage_context.docstore.docs.keys()))
        print(f"ç¤ºä¾‹èŠ‚ç‚¹IDï¼š{sample_key}")
    else:
        print("è­¦å‘Šï¼šæ–‡æ¡£å­˜å‚¨ä¸ºç©ºï¼Œè¯·æ£€æŸ¥èŠ‚ç‚¹æ·»åŠ é€»è¾‘ï¼")
    
    
    return index

# ================== ç•Œé¢ç»„ä»¶ ==================
def init_chat_interface():
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    for msg in st.session_state.messages:
        role = msg["role"]
        content = msg.get("cleaned", msg["content"])  # ä¼˜å…ˆä½¿ç”¨æ¸…ç†åçš„å†…å®¹
        
        with st.chat_message(role):
            st.markdown(content)
            
            # å¦‚æœæ˜¯åŠ©æ‰‹æ¶ˆæ¯ä¸”åŒ…å«æ€ç»´é“¾
            if role == "assistant" and msg.get("think"):
                with st.expander("ğŸ“ æ¨¡å‹æ€è€ƒè¿‡ç¨‹ï¼ˆå†å²å¯¹è¯ï¼‰"):
                    for think_content in msg["think"]:
                        st.markdown(f'<span style="color: #808080">{think_content.strip()}</span>',
                                  unsafe_allow_html=True)
            
            # å¦‚æœæ˜¯åŠ©æ‰‹æ¶ˆæ¯ä¸”æœ‰å‚è€ƒä¾æ®ï¼ˆéœ€è¦ä¿æŒåŸæœ‰å‚è€ƒä¾æ®é€»è¾‘ï¼‰
            if role == "assistant" and "reference_nodes" in msg:
                show_reference_details(msg["reference_nodes"])

def show_reference_details(nodes):
    with st.expander("æŸ¥çœ‹æ”¯æŒä¾æ®"):
        for idx, node in enumerate(nodes, 1):
            meta = node.node.metadata
            st.markdown(f"**[{idx}] {meta['full_title']}**")
            st.caption(f"æ¥æºæ–‡ä»¶ï¼š{meta['source_file']} | æ³•å¾‹åç§°ï¼š{meta['law_name']}")
            st.markdown(f"ç›¸å…³åº¦ï¼š`{node.score:.4f}`")
            # st.info(f"{node.node.text[:300]}...")
            st.info(f"{node.node.text}")

# ================== ä¸»ç¨‹åº ==================
def main():
    # ç¦ç”¨ Streamlit æ–‡ä»¶çƒ­é‡è½½
    disable_streamlit_watcher()
    st.title("âš–ï¸ æ™ºèƒ½åŠ³åŠ¨æ³•å’¨è¯¢åŠ©æ‰‹")
    st.markdown("æ¬¢è¿ä½¿ç”¨åŠ³åŠ¨æ³•æ™ºèƒ½å’¨è¯¢ç³»ç»Ÿï¼Œè¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼Œæˆ‘ä»¬å°†åŸºäºæœ€æ–°åŠ³åŠ¨æ³•å¾‹æ³•è§„ä¸ºæ‚¨è§£ç­”ã€‚")

    # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
    if "history" not in st.session_state:
        st.session_state.history = []

    embed_model, llm, reranker = init_models()

    # ä»…å½“éœ€è¦æ›´æ–°æ•°æ®æ—¶æ‰§è¡Œ
    if not Path(Config.VECTOR_DB_DIR).exists():
        with st.spinner("æ­£åœ¨æ„å»ºçŸ¥è¯†åº“..."):
            raw_data = load_and_validate_json_files(Config.DATA_DIR)
            nodes = create_nodes(raw_data)
    else:
        nodes = None  # å·²æœ‰æ•°æ®æ—¶ä¸åŠ è½½


    #print("\nåˆå§‹åŒ–å‘é‡å­˜å‚¨...")
    #start_time = time.time()
    #index = init_vector_store(nodes)
    #print(f"ç´¢å¼•åŠ è½½è€—æ—¶ï¼š{time.time()-start_time:.2f}s")

    # åˆ›å»ºæŸ¥è¯¢å¼•æ“
    # TODO: query_engineå’Œretrieverçš„åŒºåˆ«æ˜¯ä»€ä¹ˆ
    #query_engine = index.as_query_engine(
    #    similarity_top_k=Config.TOP_K,
    #    # text_qa_template=response_template,
    #    verbose=True
    #)

    index = init_vector_store(nodes)
    retriever = index.as_retriever(
        similarity_top_k=Config.TOP_K,
        vector_store_query_mode="hybrid",
        alpha=0.5,
    )
    response_synthesizer = get_response_synthesizer(
        # text_qa_template=response_template,
        verbose=True
    )

    # èŠå¤©ç•Œé¢
    init_chat_interface()

    if prompt := st.chat_input("è¯·è¾“å…¥åŠ³åŠ¨æ³•ç›¸å…³é—®é¢˜"):
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # å¤„ç†æŸ¥è¯¢
        with st.spinner("æ­£åœ¨åˆ†æé—®é¢˜..."):
            start_time = time.time()
            
            # æ£€ç´¢æµç¨‹
            initial_nodes = retriever.retrieve(prompt)
            reranked_nodes = reranker.postprocess_nodes(initial_nodes, query_str=prompt)
            
            # è¿‡æ»¤èŠ‚ç‚¹
            MIN_RERANK_SCORE = 0.4
            filtered_nodes = [node for node in reranked_nodes if node.score > MIN_RERANK_SCORE]
            
            if not filtered_nodes:
                response_text = "âš ï¸ æœªæ‰¾åˆ°ç›¸å…³æ³•å¾‹æ¡æ–‡ï¼Œè¯·å°è¯•è°ƒæ•´é—®é¢˜æè¿°æˆ–å’¨è¯¢ä¸“ä¸šå¾‹å¸ˆã€‚"
            else:
                # ç”Ÿæˆå›ç­”
                response = response_synthesizer.synthesize(prompt, nodes=filtered_nodes)
                response_text = response.response
            
            # æ˜¾ç¤ºå›ç­”
            with st.chat_message("assistant"):
                # æå–æ€ç»´é“¾å†…å®¹å¹¶æ¸…ç†å“åº”æ–‡æœ¬
                think_contents = re.findall(r'<think>(.*?)</think>', response_text, re.DOTALL)
                cleaned_response = re.sub(r'<think>.*?</think>', '', response_text, flags=re.DOTALL).strip()
                
                # æ˜¾ç¤ºæ¸…ç†åçš„å›ç­”
                st.markdown(cleaned_response)
                
                # å¦‚æœæœ‰æ€ç»´é“¾å†…å®¹åˆ™æ˜¾ç¤º
                if think_contents:
                    with st.expander("ğŸ“ æ¨¡å‹æ€è€ƒè¿‡ç¨‹ï¼ˆç‚¹å‡»å±•å¼€ï¼‰"):
                        for content in think_contents:
                            st.markdown(f'<span style="color: #808080">{content.strip()}</span>', 
                                      unsafe_allow_html=True)
                
                # æ˜¾ç¤ºå‚è€ƒä¾æ®ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
                show_reference_details(filtered_nodes[:3])

            # æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯åˆ°å†å²ï¼ˆéœ€è¦å­˜å‚¨åŸå§‹å“åº”ï¼‰
            st.session_state.messages.append({
                "role": "assistant",
                "content": response_text,  # ä¿ç•™åŸå§‹å“åº”
                "cleaned": cleaned_response,  # å­˜å‚¨æ¸…ç†åçš„æ–‡æœ¬
                "think": think_contents  # å­˜å‚¨æ€ç»´é“¾å†…å®¹
            })
    ## ç¤ºä¾‹æŸ¥è¯¢
    #while True:
    #    question = input("\nè¯·è¾“å…¥åŠ³åŠ¨æ³•ç›¸å…³é—®é¢˜ï¼ˆè¾“å…¥qé€€å‡ºï¼‰: ")
    #    if question.lower() == 'q':
    #        break

    #    # æµç¨‹: æ£€ç´¢->é‡æ’åº->è¿‡æ»¤->å›ç­”
    #    start_time = time.time()

    #    # 1. æ£€ç´¢
    #    initial_nodes = retriever.retrieve(question)
    #    retrieval_time = time.time() - start_time
    #    
    #    # 2. é‡æ’åº
    #    reranked_nodes = reranker.postprocess_nodes(
    #        initial_nodes, 
    #        query_str=question
    #    )
    #    rerank_time = time.time() - start_time - retrieval_time

    #    # 3. è¿‡æ»¤
    #    MIN_RERANK_SCORE = 0.4
    #    # æ‰§è¡Œè¿‡æ»¤
    #    filtered_nodes = [
    #        node for node in reranked_nodes 
    #        if node.score > MIN_RERANK_SCORE
    #    ]

    #    #ä¸€èˆ¬å¯¹æ¨¡å‹çš„å›å¤åšé™åˆ¶å°±ä»filtered_nodesçš„è¿”å›å€¼ä¸‹æ‰‹
    #    print("åŸå§‹åˆ†æ•°æ ·ä¾‹ï¼š",[node.score for node in reranked_nodes[:3]])
    #    print("é‡æ’åºè¿‡æ»¤åçš„ç»“æœï¼š",filtered_nodes)
    #    # ç©ºç»“æœå¤„ç†
    #    if not filtered_nodes:
    #        print("ä½ çš„é—®é¢˜æœªåŒ¹é…åˆ°ç›¸å…³èµ„æ–™ï¼")
    #        continue

    #    # 4. åˆæˆç­”æ¡ˆï¼ˆä½¿ç”¨è¿‡æ»¤åçš„èŠ‚ç‚¹ï¼‰
    #    response = response_synthesizer.synthesize(
    #        question, 
    #        nodes=filtered_nodes  # ä½¿ç”¨è¿‡æ»¤åçš„èŠ‚ç‚¹
    #    )
    #    synthesis_time = time.time() - start_time - retrieval_time - rerank_time

    #    # æ‰“å°æ˜¾ç¤º
    #    print(f"\næ™ºèƒ½åŠ©æ‰‹å›ç­”ï¼š\n{response.response}")
    #    print("\næ”¯æŒä¾æ®ï¼š")
    #    for idx, node in enumerate(reranked_nodes, 1):
    #        # å…¼å®¹æ–°ç‰ˆAPIçš„åˆ†æ•°è·å–æ–¹å¼
    #        initial_score = node.metadata.get('initial_score', node.score)  # è·å–åˆå§‹åˆ†æ•°
    #        rerank_score = node.score  # é‡æ’åºåçš„åˆ†æ•°
    #    
    #        meta = node.node.metadata
    #        print(f"\n[{idx}] {meta['full_title']}")
    #        print(f"  æ¥æºæ–‡ä»¶ï¼š{meta['source_file']}")
    #        print(f"  æ³•å¾‹åç§°ï¼š{meta['law_name']}")
    #        print(f"  åˆå§‹ç›¸å…³åº¦ï¼š{node.node.metadata.get('initial_score', 0):.4f}")  # å®‰å…¨è®¿é—®
    #        print(f"  é‡æ’åºå¾—åˆ†ï¼š{getattr(node, 'score', 0):.4f}")  # å…¼å®¹å±æ€§è®¿é—®
    #        print(f"  æ¡æ¬¾å†…å®¹ï¼š{node.node.text[:100]}...")
    #    
    #    print(f"\n[æ€§èƒ½åˆ†æ] æ£€ç´¢: {retrieval_time:.2f}s | é‡æ’åº: {rerank_time:.2f}s | åˆæˆ: {synthesis_time:.2f}s")

if __name__ == "__main__":
    main()